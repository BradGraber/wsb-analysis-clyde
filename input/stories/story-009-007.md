---
id: story-009-007
epic: epic-009
title: Write Prediction-Based Accuracy Tests
priority: medium
story_points: [TBD]
source_requirements: [TRUST-SENTIMENT-ACCURACY, TRUST-COLD-START]
dependencies: [story-009-004]
blocks: []
---

# Story: Write Prediction-Based Accuracy Tests

## Description
Write 7 unit/integration tests covering the prediction-based accuracy calculation and EMA update logic. Tests cover: basic positive accuracy (bullish prediction with positive return is correct), basic negative accuracy (bearish prediction with negative return is correct), HITL override reversing auto-determined accuracy, HITL 'excluded' skipping EMA update, cold start behavior (NULL avg_sentiment_accuracy defaults to 0.5 in trust calculation and direct-set on first resolution), sequential EMA convergence across 3+ predictions, and neutral handling (simulated_return_pct within a neutral zone is still classified by the > 0 rule).

## Acceptance Criteria
- [ ] Test 1 (positive accuracy): A bullish prediction (option_type='call') with positive simulated_return_pct sets is_correct=TRUE and EMA updates author accuracy upward
- [ ] Test 2 (negative accuracy): A bearish prediction (option_type='put') with negative simulated_return_pct sets is_correct=FALSE and EMA updates author accuracy downward
- [ ] Test 3 (HITL override): A prediction auto-determined as is_correct=FALSE but with hitl_override='correct' results in accuracy being updated as if correct (1.0 new value in EMA)
- [ ] Test 4 (HITL excluded): A prediction with hitl_override='excluded' does NOT modify author.avg_sentiment_accuracy at all -- value remains unchanged before and after
- [ ] Test 5 (cold start): An author with avg_sentiment_accuracy=NULL has trust score calculated using 0.5 default for accuracy component; after first prediction resolves, avg_sentiment_accuracy is set directly (not via EMA) to 1.0 or 0.0
- [ ] Test 6 (sequential EMA): 3 or more predictions resolving sequentially for the same author produce correct EMA convergence -- each step applies (1-0.30)*old + 0.30*new, verified to 3 decimal places
- [ ] Test 7 (near-zero return): A prediction with simulated_return_pct very close to zero but positive (e.g., 0.001) is classified as is_correct=TRUE; a prediction with exactly 0.0 return is classified as is_correct=FALSE (per the > 0 rule)

## Technical Notes
- Tests validate the logic from story-009-004 (prediction resolution and accuracy update)
- EMA formula: new_accuracy = (1 - accuracy_ema_weight) * old + accuracy_ema_weight * (1.0 if correct else 0.0), with accuracy_ema_weight from system_config (default 0.30)
- Cold start behavior specified in PRD Appendix F.4.2-F.4.3 (lines 3052-3077)
- The neutral threshold mention in epic description refers to the accuracy determination rule: is_correct = simulated_return_pct > 0. There is no separate neutral threshold -- the > 0 boundary is the only decision point
- Test fixtures should include: mock author records with various accuracy states, mock prediction records with prediction_exits, mock system_config values
- Tests should be runnable independently and not require Schwab API access

## Dependencies
- Depends on: story-009-004 (the prediction resolution and accuracy logic being tested)
- Blocks: none
