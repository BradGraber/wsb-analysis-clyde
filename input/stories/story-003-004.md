---
id: story-003-004
epic: epic-003
title: Implement Concurrent AI Request Batching
priority: high
story_points: [TBD]
source_requirements: [PIPE-017, AI-INDIVIDUAL]
dependencies: [story-003-002, story-003-003]
blocks: [story-003-005]
---

# Story: Implement Concurrent AI Request Batching

## Description
Process AI analysis requests in batches of 5 using ThreadPoolExecutor with 5 concurrent workers. Each comment is analyzed individually (1 comment per API call per AI-INDIVIDUAL). Comments are grouped into batches of 5, submitted concurrently, and results are collected before the next batch begins. Thread-safe batch management ensures no race conditions.

## Acceptance Criteria
- [ ] ThreadPoolExecutor is configured with max_workers=5 for concurrent OpenAI API calls
- [ ] Comments are processed in batches of 5: all 5 requests in a batch are submitted concurrently, and all 5 results are collected before proceeding to the next batch
- [ ] Each API call processes exactly 1 comment (per AI-INDIVIDUAL decision -- no multi-comment prompts)
- [ ] If fewer than 5 comments remain in the final batch, only that many workers are used (no empty/padding calls)
- [ ] Batch processing is thread-safe: no shared mutable state between concurrent workers except the results collection
- [ ] Each worker receives a complete prompt (from story-003-002) for its assigned comment and returns the raw API response
- [ ] If one worker in a batch raises an exception, the remaining workers complete normally; the failed comment is logged as skipped with the exception details
- [ ] After each batch completes, a progress counter is updated (completed_comments += batch_size) for progress reporting by the pipeline orchestrator
- [ ] The batch orchestrator tracks which comments were sent to which workers for error attribution

## Technical Notes
- ThreadPoolExecutor was chosen over asyncio per planning decision qc-sd-002 for simplicity with the synchronous openai SDK
- Batch size of 5 aligns with the commit granularity in story-003-008 (commit per batch of 5)
- Expected throughput: ~500 comments / 5 per batch = 100 batches per run; at ~1-2s per API call, this is ~2-4 minutes total
- Thread safety: each worker gets its own prompt string and returns its result; no shared state during execution
- Failed individual requests within a batch should not cancel the other 4 workers (use concurrent.futures.as_completed or wait with return_when=ALL_COMPLETED)
- PRD reference: Section 3.2.2 Phase 3 step 2 bullet 3, PIPE-017, AI-INDIVIDUAL, Appendix E.8

## Dependencies
- Depends on: story-003-002 (prompts must be engineered), story-003-003 (dedup filters the comment list first)
- Blocks: story-003-005 (response parsing receives results from the concurrent workers)
