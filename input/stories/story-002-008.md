---
id: story-002-008
epic: epic-002
title: Implement Comment Deduplication and Storage
priority: high
story_points: [TBD]
source_requirements: [FR-038, PIPE-007, PIPE-015, CFG-INTERNAL-DATA, ERR-REDDIT-OUTAGE, ERR-DEDUP]
dependencies: [story-002-007]
blocks: [story-002-009]
---

# Story: Implement Comment Deduplication and Storage

## Description
Implement comment deduplication by checking reddit_id before processing (reusing stored annotations for existing comments), store posts and selected comments with full metadata in the database, and handle Reddit outages as Tier 1 errors (log + HTTP 503). Define the ProcessedComment, ProcessedPost, and ParentChainEntry data structures as concrete Python classes.

## Acceptance Criteria
- [ ] Before processing a comment, check the database for an existing record by reddit_id (UNIQUE constraint on comments.reddit_id)
- [ ] If a comment already exists in the database: UPDATE the comment's analysis_run_id to the current run ID and preserve stored annotations (sentiment, sarcasm_detected, has_reasoning, confidence, tickers). Note: the decision to skip the AI API call is made in Phase 3 (story-003-003), not here.
- [ ] New posts are inserted into the reddit_posts table with all fields: reddit_id, title, selftext, upvotes, total_comments, image_url, image_analysis
- [ ] New comments are inserted into the comments table with all metadata fields including post_id (FK), reddit_id (UNIQUE), author, body, score, depth, created_utc, priority_score, financial_score, author_trust_score, parent_chain (stored as JSON)
- [ ] ProcessedComment, ProcessedPost, and ParentChainEntry are implemented as Python dataclasses or Pydantic models matching PRD Appendix D field definitions exactly (11, 8, and 4 fields respectively)
- [ ] Reddit outages (PRAW exceptions during any fetch operation) are caught and result in: structlog error log with exception details, HTTP 503 response returned to caller per ERR-REDDIT-OUTAGE Tier 1 handling
- [ ] Database operations use transactions to ensure posts and their comments are stored atomically
- [ ] If a transaction fails during post/comment storage, the error is logged and the pipeline continues to the next post (no crash)

## Technical Notes
- Phase 2 dedup responsibility: INSERT new comments, UPDATE existing comments' analysis_run_id. Phase 2 does NOT decide whether to skip AI â€” that decision belongs to Phase 3 (story-003-003)
- On re-runs, ~60-80% of comments may already exist; the cost savings from skipping AI calls are realized in Phase 3
- Deduplicated comments retain their original author_trust_score snapshot from first analysis (acceptable per PRD note)
- ParentChainEntry fields: id (string), body (string), depth (int), author (string) per Appendix D.3
- ProcessedComment fields: reddit_id, post_id, author, body, score, depth, created_utc, priority_score, financial_score, author_trust_score, parent_chain per Appendix D.1
- ProcessedPost fields: reddit_id, title, selftext, upvotes, total_comments, image_url, image_analysis, comments per Appendix D.2
- PRD reference: Section 3.2.2 Phase 1 step 6, Phase 3 step 1, Appendix D

## Dependencies
- Depends on: story-002-007 (scored and selected comments ready for storage)
- Blocks: story-002-009 (tests need storage implementation complete)
