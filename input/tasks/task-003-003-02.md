---
id: task-003-003-02
story: story-003-003
title: Implement dedup decision logic with analysis_run_id update
complexity: 2
---

# Task: Implement dedup decision logic with analysis_run_id update

## Description
Implement the deduplication decision function that, given the batch dedup query results (task-003-003-01), partitions comments into two groups: (1) already-analyzed comments that skip AI and get their analysis_run_id updated, and (2) new/incomplete comments that proceed to AI analysis. For deduplicated comments, UPDATE the analysis_run_id to the current run so they appear in Phase 4 signal detection queries.

## Acceptance Criteria
- [ ] `partition_for_analysis(comments, dedup_results, current_run_id)` returns (skip_list, analyze_list)
- [ ] skip_list contains comments with existing annotations; analyze_list contains comments needing AI
- [ ] For each comment in skip_list: UPDATE comments SET analysis_run_id = current_run_id WHERE reddit_id = ?
- [ ] Deduplicated comments retain their original author_trust_score snapshot (no re-lookup)
- [ ] skip_list entries carry their existing annotations for downstream use in Phase 4
- [ ] structlog info log: "Deduplicated {n} comments, {m} new comments for AI analysis"
- [ ] Unit test: correctly partitions mixed batch (some existing, some new)
- [ ] Unit test: updates analysis_run_id for deduplicated comments

## Technical Notes
- The UPDATE can be batched: UPDATE comments SET analysis_run_id = ? WHERE reddit_id IN (?,...)
- This function is called once per pipeline run with the full list of prioritized comments
- On re-runs, ~60-80% of comments are deduplicated, saving ~$27-36/month in AI costs
- PRD reference: PIPE-015, Section 3.2.2 Phase 3 step 1

## Dependencies
- Depends on: task-003-003-01 (dedup query results)
- Blocks: task-003-004-01 (concurrent batching receives only the analyze_list)
