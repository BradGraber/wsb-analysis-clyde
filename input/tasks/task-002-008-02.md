---
id: task-002-008-02
story: story-002-008
title: Implement comment deduplication check and analysis_run_id update
complexity: 2
---

# Task: Implement comment deduplication check and analysis_run_id update

## Description
Create a `check_duplicates(comment_ids: list[str], run_id: int) -> set[str]` function that queries the comments table for existing reddit_ids, updates their analysis_run_id to the current run, and returns the set of duplicate reddit_ids. This enables downstream Phase 3 (epic-003) to skip AI API calls for already-analyzed comments, saving cost on re-runs where 60-80% of comments may already exist.

## Acceptance Criteria
- [ ] Queries comments table for existing records matching the provided reddit_id list
- [ ] Returns a set of reddit_ids that already exist in the database (duplicates)
- [ ] For each duplicate, UPDATEs the comment's analysis_run_id to the current run ID
- [ ] Existing comments' stored annotations (sentiment, sarcasm_detected, has_reasoning, confidence, tickers) are preserved unchanged
- [ ] New (non-duplicate) reddit_ids are NOT in the returned set
- [ ] Efficiently handles batches (uses IN clause, respects SQLite's ~999 parameter limit)
- [ ] Unit test: mix of new and existing comments correctly identifies duplicates
- [ ] Unit test: duplicate comments have analysis_run_id updated
- [ ] Unit test: empty input list returns empty set

## Technical Notes
- Phase 2 (this epic) handles INSERT for new + UPDATE run_id for existing
- Phase 3 (epic-003 story-003-003) decides whether to skip AI calls based on this dedup result
- On re-runs, ~60-80% of comments may be duplicates; the cost savings are realized in Phase 3
- Batch the IN clause for >999 comments (SQLite parameter limit)
- Uses get_connection() from task-001-004-01
- PRD reference: FR-038, PIPE-015, ERR-DEDUP

## Dependencies
- Depends on: task-001-004-01 (DB connection), task-001-001-01 (comments table schema)
- Blocks: task-002-008-03 (storage uses dedup results)
